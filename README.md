# llama
some notes about llama repos

|Repo|Usage|Comment|
|---|---|---|
|[LLaMA-Factory](./LLaMA-Factory)|Fine-tuning LLMs framework.||
|[llama.cpp]()|Inference of Meta's LLaMA model (and others) in pure C/C++||
|[llamafile](llamafile/README.md)|llamafile lets you distribute and run LLMs with a single file.||
|[llamaindex]()|LlamaIndex (GPT Index) is a data framework for your LLM application.||
|[ollama]()|Run Llama 3.1, Phi 3, Mistral, Gemma 2, and other models. Customize and create your own.||
|[llama]()|Inference code for Llama models.||

# references

[LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)

[llama.cpp](https://github.com/ggerganov/llama.cpp)

[llamafile](https://github.com/Mozilla-Ocho/llamafile)

[llamaindex](https://github.com/run-llama/llama_index)

[ollama](https://ollama.com/)

[llama](https://github.com/meta-llama/llama)